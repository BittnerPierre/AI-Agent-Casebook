# Prompt Engineering for Developers - Chapter 1: Basics

## Introduction to Prompt Engineering

Welcome to this comprehensive module on prompt engineering for developers. In this session, we'll explore the fundamental techniques that transform how we interact with large language models to build robust, reliable AI applications.

Prompt engineering is the practice of designing and optimizing text inputs to elicit desired responses from language models. As developers, understanding these techniques is crucial for building production-ready AI systems that behave predictably and effectively.

## Understanding System vs User Prompts

### System Prompts: Setting the Foundation

System prompts establish the context and behavioral guidelines for your AI assistant. Think of them as configuration files for language model behavior.

**Example System Prompt:**
```
You are a helpful coding assistant specialized in Python development. You provide clear, well-commented code examples and explain complex concepts in simple terms. Always consider best practices for code safety and performance.
```

System prompts are particularly powerful because they:
- Set the overall tone and personality
- Define expertise areas and limitations
- Establish response format preferences
- Configure safety and ethical guidelines

### User Prompts: The Specific Request

User prompts contain the actual task or question. They should be clear, specific, and provide necessary context.

**Example User Prompt:**
```
Write a Python function that calculates the moving average of a list of numbers with a configurable window size. Include error handling for edge cases.
```

### Best Practices for Prompt Structure

1. **Be Specific**: Vague prompts lead to unpredictable outputs
2. **Provide Context**: Include relevant background information
3. **Set Expectations**: Specify desired format, length, or style
4. **Use Examples**: Show the model what good output looks like

## Zero-Shot and Few-Shot Techniques

### Zero-Shot Prompting

Zero-shot prompting relies on the model's pre-trained knowledge without providing examples. This technique works well for common tasks that the model has seen during training.

**Example Zero-Shot Prompt:**
```
Classify the sentiment of this customer review as positive, negative, or neutral:
"The product arrived quickly but the quality was disappointing."
```

Zero-shot prompting is ideal when:
- The task is straightforward and common
- You need quick results without setup
- The domain is well-represented in training data

### Few-Shot Prompting

Few-shot prompting provides examples to guide the model's understanding of the task format and requirements.

**Example Few-Shot Prompt:**
```
Classify customer feedback sentiment:

Example 1:
Feedback: "Excellent service, very satisfied!"
Sentiment: Positive

Example 2:
Feedback: "Delivery was late and item was damaged"
Sentiment: Negative

Example 3:
Feedback: "Product works as expected"
Sentiment: Neutral

Now classify:
Feedback: "Good quality but expensive for what it is"
Sentiment:
```

Few-shot prompting excels when:
- The task format needs clarification
- You need consistent output structure
- The domain requires specific expertise
- Edge cases need explicit handling

## Chain-of-Thought Prompting

Chain-of-Thought (CoT) prompting encourages the model to show its reasoning process step-by-step. This technique dramatically improves performance on complex reasoning tasks.

### Basic Chain-of-Thought

**Example:**
```
Question: A restaurant serves 150 customers on weekdays and 40% more on weekends. If they're open 6 days a week (5 weekdays, 1 weekend day), how many customers do they serve per week?

Let me think through this step by step:

1. Weekday customers: 150 per day
2. Weekend increase: 40% of 150 = 60 additional customers
3. Weekend customers: 150 + 60 = 210 per day
4. Weekly total: (5 × 150) + (1 × 210) = 750 + 210 = 960 customers
```

### Advanced CoT Techniques

**Self-Consistency**: Generate multiple reasoning paths and choose the most common answer.

**Tree of Thoughts**: Explore multiple reasoning branches simultaneously.

**Verification**: Ask the model to double-check its reasoning.

## ReAct Prompting (Reasoning + Acting)

ReAct combines reasoning with action execution, making it powerful for multi-step problem solving and tool usage.

### ReAct Pattern Structure

```
Thought: [Reasoning about what to do next]
Action: [Specific action to take]
Observation: [Result of the action]
Thought: [Reasoning about the observation]
Action: [Next action based on new information]
...
```

### ReAct Example: Web Research Task

```
Question: What's the current stock price of Apple and how has it changed this week?

Thought: I need to get the current stock price for Apple (AAPL) and compare it to prices from earlier this week.

Action: Search for "Apple AAPL current stock price"

Observation: Apple Inc. (AAPL) is currently trading at $185.42, up 2.3% from yesterday.

Thought: I have the current price, but I need historical data from this week to show the change.

Action: Search for "Apple AAPL stock price this week chart"

Observation: Weekly data shows: Monday $180.15, Tuesday $182.30, Wednesday $181.95, Thursday $183.67, Friday $185.42.

Thought: Now I can calculate the weekly change and provide a comprehensive answer.

Answer: Apple (AAPL) is currently trading at $185.42. This week, the stock has gained $5.27 or 2.93% from Monday's opening price of $180.15, showing positive momentum throughout the week.
```

## Basic Prompt Optimization Patterns

### Template-Based Prompts

Create reusable prompt templates for consistent results:

```python
ANALYSIS_TEMPLATE = """
Analyze the following {data_type} for {analysis_goal}:

Data: {input_data}

Please provide:
1. Key findings (top 3-5 insights)
2. Patterns or trends observed
3. Recommendations based on analysis
4. Confidence level in findings (High/Medium/Low)

Format your response as a structured report.
"""
```

### Conditional Prompting

Adapt prompts based on input characteristics:

```python
def create_review_prompt(review_length):
    if review_length < 50:
        return "Provide a brief sentiment analysis for this short review:"
    elif review_length < 200:
        return "Analyze sentiment and key themes in this review:"
    else:
        return "Provide comprehensive sentiment analysis, key themes, and specific concerns from this detailed review:"
```

### Error Handling in Prompts

Build robustness into your prompts:

```
Analyze the customer feedback below. If the input is unclear, too short, or not actually customer feedback, respond with "INVALID_INPUT" and explain why.

For valid feedback, provide:
- Sentiment: [Positive/Negative/Neutral]
- Key themes: [List main topics]
- Priority level: [High/Medium/Low]

Customer feedback: {input_text}
```

## Practical Implementation Examples

### Code Generation Prompts

**Basic Code Generation:**
```
Write a Python function that:
1. Takes a list of dictionaries as input
2. Filters items where 'status' equals 'active'
3. Sorts by 'priority' in descending order
4. Returns the top 5 items
5. Include proper error handling and docstring
```

**Advanced Code Generation with Context:**
```
You are an expert Python developer working on a web application using Flask and SQLAlchemy.

Context: We have a User model with fields: id, username, email, created_at, is_active
Task: Write a function to get all active users registered in the last 30 days, sorted by registration date.

Requirements:
- Use SQLAlchemy query syntax
- Include proper error handling
- Add type hints
- Write comprehensive docstring
- Handle edge cases (no users found, database errors)
```

### Data Analysis Prompts

**Structured Analysis:**
```
Analyze this dataset and provide insights:

Dataset: {data_description}
Sample data: {sample_rows}

Provide analysis in this format:

## Data Overview
- Number of records: [count]
- Key columns: [list]
- Data quality issues: [list any problems]

## Key Insights
1. [Most important finding]
2. [Second most important finding]
3. [Third most important finding]

## Recommendations
- [Actionable recommendation 1]
- [Actionable recommendation 2]

## Technical Notes
- Analysis method: [describe approach]
- Confidence level: [High/Medium/Low]
- Limitations: [any caveats]
```

### Documentation Generation

**API Documentation:**
```
Generate comprehensive API documentation for this function:

Function: {function_code}

Include:
1. Clear description of purpose
2. Parameter details with types and constraints
3. Return value specification
4. Usage examples with different scenarios
5. Error conditions and handling
6. Performance considerations
7. Related functions or dependencies

Use standard API documentation format.
```

## Key Takeaways

1. **System prompts establish context**, user prompts define tasks
2. **Few-shot examples improve consistency** for complex or novel tasks
3. **Chain-of-thought reasoning enhances accuracy** on complex problems
4. **ReAct patterns enable tool usage** and multi-step problem solving
5. **Template-based approaches ensure consistency** across applications
6. **Error handling prevents invalid outputs** and improves reliability

Understanding these fundamental prompt engineering techniques provides the foundation for building robust AI applications. These patterns form the building blocks for more advanced optimization strategies and production deployment considerations.