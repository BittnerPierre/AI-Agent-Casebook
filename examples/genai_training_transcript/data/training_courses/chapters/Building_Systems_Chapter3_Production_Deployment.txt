# Building Systems with the ChatGPT API - Chapter 3: Production Deployment

## Error Handling and Resilience

### API Error Types and Handling

**Comprehensive Error Handling:**
```python
import time
import random
from typing import Optional, Union

class APIErrorHandler:
    def __init__(self, max_retries: int = 3, base_delay: float = 1.0):
        self.max_retries = max_retries
        self.base_delay = base_delay
    
    def with_retry(self, func, *args, **kwargs):
        """Execute function with exponential backoff retry"""
        for attempt in range(self.max_retries + 1):
            try:
                return func(*args, **kwargs)
            except openai.RateLimitError as e:
                if attempt == self.max_retries:
                    raise e
                
                # Extract retry-after header if available
                retry_after = getattr(e, 'retry_after', None)
                delay = retry_after if retry_after else (self.base_delay * (2 ** attempt))
                
                print(f"Rate limit hit. Retrying in {delay} seconds...")
                time.sleep(delay)
                
            except openai.APIConnectionError as e:
                if attempt == self.max_retries:
                    raise e
                
                delay = self.base_delay * (2 ** attempt) + random.uniform(0, 1)
                print(f"Connection error. Retrying in {delay:.2f} seconds...")
                time.sleep(delay)
                
            except openai.APIError as e:
                # Don't retry on client errors (4xx)
                if hasattr(e, 'status_code') and 400 <= e.status_code < 500:
                    raise e
                
                if attempt == self.max_retries:
                    raise e
                
                delay = self.base_delay * (2 ** attempt)
                print(f"API error. Retrying in {delay} seconds...")
                time.sleep(delay)
        
        raise Exception("Max retries exceeded")
```

**Robust API Client:**
```python
class RobustOpenAIClient:
    def __init__(self, api_key: str, default_model: str = "gpt-3.5-turbo"):
        self.client = openai.OpenAI(api_key=api_key)
        self.default_model = default_model
        self.error_handler = APIErrorHandler()
    
    def safe_completion(self, messages: list, **kwargs) -> Optional[str]:
        """Safe completion with error handling and fallbacks"""
        try:
            response = self.error_handler.with_retry(
                self.client.chat.completions.create,
                model=kwargs.get('model', self.default_model),
                messages=messages,
                **{k: v for k, v in kwargs.items() if k != 'model'}
            )
            return response.choices[0].message.content
        except openai.RateLimitError:
            return "Service temporarily unavailable due to rate limits. Please try again later."
        except openai.APIConnectionError:
            return "Unable to connect to the service. Please check your internet connection."
        except openai.InvalidRequestError as e:
            return f"Invalid request: {e}"
        except Exception as e:
            return f"An unexpected error occurred: {e}"
    
    def streaming_completion(self, messages: list, **kwargs):
        """Streaming completion with error handling"""
        try:
            stream = self.client.chat.completions.create(
                model=kwargs.get('model', self.default_model),
                messages=messages,
                stream=True,
                **{k: v for k, v in kwargs.items() if k not in ['model', 'stream']}
            )
            
            for chunk in stream:
                if chunk.choices[0].delta.content is not None:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            yield f"Error in streaming: {e}"
```

### Input Validation and Sanitization

**Request Validation:**
```python
from pydantic import BaseModel, validator
from typing import List, Optional

class ChatRequest(BaseModel):
    message: str
    max_tokens: Optional[int] = 1000
    temperature: Optional[float] = 0.7
    model: Optional[str] = "gpt-3.5-turbo"
    
    @validator('message')
    def validate_message(cls, v):
        if not v or not v.strip():
            raise ValueError('Message cannot be empty')
        if len(v) > 10000:
            raise ValueError('Message too long (max 10000 characters)')
        return v.strip()
    
    @validator('max_tokens')
    def validate_max_tokens(cls, v):
        if v is not None and (v < 1 or v > 4000):
            raise ValueError('max_tokens must be between 1 and 4000')
        return v
    
    @validator('temperature')
    def validate_temperature(cls, v):
        if v is not None and (v < 0 or v > 2):
            raise ValueError('temperature must be between 0 and 2')
        return v

class ChatService:
    def __init__(self, client: RobustOpenAIClient):
        self.client = client
    
    def process_chat_request(self, request: ChatRequest) -> dict:
        try:
            # Validate request
            validated_request = ChatRequest(**request.dict())
            
            # Prepare messages
            messages = [{"role": "user", "content": validated_request.message}]
            
            # Get completion
            response = self.client.safe_completion(
                messages=messages,
                max_tokens=validated_request.max_tokens,
                temperature=validated_request.temperature,
                model=validated_request.model
            )
            
            return {
                "success": True,
                "response": response,
                "usage": {"prompt_tokens": len(validated_request.message) // 4}  # Rough estimate
            }
            
        except ValueError as e:
            return {"success": False, "error": f"Validation error: {e}"}
        except Exception as e:
            return {"success": False, "error": f"Processing error: {e}"}
```

## Architecture Patterns for Production Systems

### Service Layer Architecture

**API Service Layer:**
```python
from fastapi import FastAPI, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

app = FastAPI(title="ChatGPT Integration Service", version="1.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatGPTService:
    def __init__(self):
        self.client = RobustOpenAIClient(
            api_key=os.getenv('OPENAI_API_KEY')
        )
        self.function_registry = FunctionRegistry()
        self._register_functions()
    
    def _register_functions(self):
        # Register available functions
        self.function_registry.register("get_current_time", {
            "name": "get_current_time",
            "description": "Get the current date and time",
            "parameters": {"type": "object", "properties": {}}
        })(self.get_current_time)
    
    def get_current_time(self):
        from datetime import datetime
        return datetime.now().isoformat()
    
    async def process_message(self, message: str, use_functions: bool = False):
        if use_functions:
            agent = FunctionCallingAgent(self.function_registry)
            return agent.chat_with_functions(message)
        else:
            return self.client.safe_completion([
                {"role": "user", "content": message}
            ])

# Initialize service
chat_service = ChatGPTService()

@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    try:
        response = await chat_service.process_message(
            request.message,
            use_functions=False
        )
        return {"response": response}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/chat/functions")
async def chat_with_functions_endpoint(request: ChatRequest):
    try:
        response = await chat_service.process_message(
            request.message,
            use_functions=True
        )
        return {"response": response}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    return {"status": "healthy", "service": "ChatGPT Integration"}
```

### Async and Concurrent Processing

**Async Processing Patterns:**
```python
import asyncio
import aiohttp
from typing import List, Dict

class AsyncChatGPTClient:
    def __init__(self, api_key: str, max_concurrent: int = 5):
        self.api_key = api_key
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.session = None
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def async_completion(self, messages: List[Dict], **kwargs):
        async with self.semaphore:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": kwargs.get("model", "gpt-3.5-turbo"),
                "messages": messages,
                "max_tokens": kwargs.get("max_tokens", 1000),
                "temperature": kwargs.get("temperature", 0.7)
            }
            
            async with self.session.post(
                "https://api.openai.com/v1/chat/completions",
                headers=headers,
                json=data
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    return result["choices"][0]["message"]["content"]
                else:
                    error_text = await response.text()
                    raise Exception(f"API error {response.status}: {error_text}")
    
    async def batch_completions(self, message_batches: List[List[Dict]]):
        """Process multiple completions concurrently"""
        tasks = [
            self.async_completion(messages) 
            for messages in message_batches
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        return results
```

### Monitoring and Observability

**Usage Tracking and Monitoring:**
```python
import logging
from dataclasses import dataclass
from datetime import datetime
import json

@dataclass
class APIUsageMetrics:
    request_id: str
    timestamp: datetime
    model: str
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int
    response_time: float
    success: bool
    error_message: str = None

class MonitoredChatGPTClient:
    def __init__(self, api_key: str):
        self.client = openai.OpenAI(api_key=api_key)
        self.metrics = []
        self.logger = logging.getLogger(__name__)
    
    def tracked_completion(self, messages: list, **kwargs):
        import time
        import uuid
        
        request_id = str(uuid.uuid4())
        start_time = time.time()
        
        try:
            response = self.client.chat.completions.create(
                model=kwargs.get('model', 'gpt-3.5-turbo'),
                messages=messages,
                **kwargs
            )
            
            end_time = time.time()
            
            # Track metrics
            usage = response.usage
            metrics = APIUsageMetrics(
                request_id=request_id,
                timestamp=datetime.now(),
                model=kwargs.get('model', 'gpt-3.5-turbo'),
                prompt_tokens=usage.prompt_tokens,
                completion_tokens=usage.completion_tokens,
                total_tokens=usage.total_tokens,
                response_time=end_time - start_time,
                success=True
            )
            
            self.metrics.append(metrics)
            self.logger.info(f"API call successful: {metrics}")
            
            return response.choices[0].message.content
            
        except Exception as e:
            end_time = time.time()
            
            # Track error metrics
            error_metrics = APIUsageMetrics(
                request_id=request_id,
                timestamp=datetime.now(),
                model=kwargs.get('model', 'gpt-3.5-turbo'),
                prompt_tokens=0,
                completion_tokens=0,
                total_tokens=0,
                response_time=end_time - start_time,
                success=False,
                error_message=str(e)
            )
            
            self.metrics.append(error_metrics)
            self.logger.error(f"API call failed: {error_metrics}")
            
            raise e
    
    def get_usage_summary(self):
        if not self.metrics:
            return {"message": "No usage data available"}
        
        successful_calls = [m for m in self.metrics if m.success]
        failed_calls = [m for m in self.metrics if not m.success]
        
        return {
            "total_calls": len(self.metrics),
            "successful_calls": len(successful_calls),
            "failed_calls": len(failed_calls),
            "success_rate": len(successful_calls) / len(self.metrics) * 100,
            "total_tokens": sum(m.total_tokens for m in successful_calls),
            "average_response_time": sum(m.response_time for m in successful_calls) / len(successful_calls) if successful_calls else 0,
            "models_used": list(set(m.model for m in self.metrics))
        }
```

## Production Deployment Best Practices

### Security Considerations

**API Key Management:**
- Store API keys in environment variables or secure vaults
- Rotate keys regularly
- Use different keys for different environments
- Implement key-level monitoring and alerting

**Input Validation:**
- Sanitize all user inputs
- Implement rate limiting per user/IP
- Validate request size and complexity
- Filter potentially harmful content

**Output Safety:**
- Implement content filtering for responses
- Monitor for potential data leakage
- Set appropriate response length limits
- Log and audit all interactions

### Performance Optimization

**Caching Strategies:**
```python
import redis
import hashlib
import json

class ResponseCache:
    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.redis_client = redis.from_url(redis_url)
        self.default_ttl = 3600  # 1 hour
    
    def get_cache_key(self, messages: list, **kwargs) -> str:
        # Create unique key based on messages and parameters
        content = json.dumps({"messages": messages, "params": kwargs}, sort_keys=True)
        return hashlib.md5(content.encode()).hexdigest()
    
    def get_cached_response(self, messages: list, **kwargs) -> str:
        cache_key = self.get_cache_key(messages, **kwargs)
        cached = self.redis_client.get(cache_key)
        return cached.decode() if cached else None
    
    def cache_response(self, messages: list, response: str, **kwargs):
        cache_key = self.get_cache_key(messages, **kwargs)
        self.redis_client.setex(cache_key, self.default_ttl, response)
```

## Key Takeaways

1. **Robust error handling is essential** for production reliability and user experience
2. **Input validation prevents security vulnerabilities** and improves system stability
3. **Service layer architecture enables scalable deployment** and maintenance
4. **Async processing improves performance** for concurrent requests
5. **Monitoring and observability help maintain system health** and optimize costs
6. **Security considerations protect both users and systems** from various threats
7. **Performance optimization reduces costs** and improves response times

Building production ChatGPT systems requires careful attention to reliability, security, and performance. The patterns covered in this chapter provide a solid foundation for deploying robust, scalable AI applications that can handle real-world demands while maintaining high standards of quality and security.